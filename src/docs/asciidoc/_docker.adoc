= Quick Start with Docker

This guide provides a hands-on look at the functionality of the Redis Enterprise Kafka Source and Sink Connectors:

* The *redis-enterprise-sink* connector reads data from a Kafka topic and writes it to a Redis stream
* The *redis-enterprise-source* connector reads data from a Redis stream and writes it to a Kafka topic

== Requirements

https://docs.docker.com/get-docker/[Docker]

== Run the example

Clone the https://github.com/RedisLabs-Field-Engineering/redis-enterprise-kafka.git[redis-enterprise-kafka] repository and execute `run.sh` in `docker` directory:

[source,bash]
----
git clone https://github.com/RedisLabs-Field-Engineering/redis-enterprise-kafka.git
cd redis-enterprise-kafka/docker
./run.sh
----

This will:

* Run `docker-compose up`
* Wait for Redis, Kafka, and Kafka Connect to be ready
* Register the Confluent Datagen Connector
* Register the Redis Enterprise Kafka Sink Connector
* Register the Redis Enterprise Kafka Source Connector
* Publish some events to Kafka via the Datagen connector
* Write the events to Redis
* Send messages to a Redis stream
* Write the Redis stream messages back into Kafka

Once running, examine the topics in the Kafka control center: http://localhost:9021/

* The `pageviews` topic should contain the 10 simple documents added, each similar to:

[source,json]
----
include::resources/pageviews.json[]
----

* The `pageviews` stream should contain the 10 change events.

Examine the stream in Redis:
[source,bash]
----
docker-compose exec redis /usr/local/bin/redis-cli
xread COUNT 10 STREAMS pageviews 0
----

Messages added to the `mystream` stream will show up in the `mystream` topic
