[[source]]
= Source Connector Guide
:name: Redis Kafka Source Connector

The {name} reads from a Redis stream and publishes messages to a Kafka topic.

== Features

The {name} includes the following features:

* <<source-at-least-once-delivery,At least once delivery>>
* <<source-tasks,Multiple tasks>>
* <<stream-reader,Stream Reader>>

[[source-at-least-once-delivery]]
=== Delivery Guarantees

The {name} can be configured to ack stream messages either automatically (at-most-once delivery) or explicitly (at-least-once delivery).
The default is at-least-once delivery.

==== At-Least-Once

In this mode, each stream message is acknowledged after it has been written to the corresponding topic.

[source,properties]
----
redis.stream.delivery=at-least-once
----

==== At-Most-Once

In this mode, stream messages are acknowledged as soon as they are read.

[source,properties]
----
redis.stream.delivery=at-most-once
----

[[source-tasks]]
=== Multiple Tasks
Use configuration property `tasks.max` to have the change stream handled by multiple tasks.
The connector splits the work based on the number of configured key patterns.
When the number of tasks is greater than the number of patterns, the number of patterns will be used instead.

//
//[[key-reader]]
//=== Key Reader
//In key reader mode, the {name} captures changes happening to keys in a Redis database and publishes keys and values to a Kafka topic.
//The data structure key will be mapped to the record key, and the value will be mapped to the record value.
//
//[IMPORTANT]
//.Supported Data Structures
//====
//The {name} supports the following data structures:
//
//* String: the Kafka record values will be strings
//* Hash: the Kafka record values will be maps (string key/value pairs)
//
//====
//
//[source,properties]
//----
//redis.keys.patterns=<glob> <1>
//topic=<topic> <2>
//----
//
//<1> Key portion of the pattern that will be used to listen to keyspace events.
For example `foo:*` translates to pubsub channel `$$__$$keyspace@0$$__$$:foo:*` and will capture changes to keys `foo:1`, `foo:2`, etc.
Use comma-separated values for multiple patterns (`foo:*,bar:*`)
//<2> Name of the destination topic.

[[stream-reader]]
=== Stream Reader
The {name} reads messages from a stream and publishes to a Kafka topic.
Reading is done through a consumer group so that <<source-tasks,multiple instances>> of the connector configured via the `tasks.max` can consume messages in a round-robin fashion.


==== Stream Message Schema

===== Key Schema

Keys are of type String and contain the stream message id.

===== Value Schema

The value schema defines the following fields:

[options="header"]
|====
|Name|Schema|Description
|id    |STRING       |Stream message ID
|stream|STRING       |Stream key
|body  |Map of STRING|Stream message body
|====

==== Configuration

[source,properties]
----
redis.stream.name=<name> <1>
redis.stream.offset=<offset> <2>
redis.stream.block=<millis> <3>
redis.stream.consumer.group=<group> <4>
redis.stream.consumer.name=<name> <5>
topic=<name> <6>
----

<1> Name of the stream to read from.
<2> https://redis.io/commands/xread#incomplete-ids[Message ID] to start reading from (default: `0-0`).
<3> Maximum https://redis.io/commands/xread[XREAD] wait duration in milliseconds (default: `100`).
<4> Name of the stream consumer group (default: `kafka-consumer-group`).
<5> Name of the stream consumer (default: `consumer-${task}`).
May contain `${task}` as a placeholder for the task id.
For example, `foo${task}` and task `123` => consumer `foo123`.
<6> Destination topic (default: `${stream}`).
May contain `${stream}` as a placeholder for the originating stream name.
For example, `redis_${stream}` and stream `orders` => topic `redis_orders`.
